{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机(SVM)\n",
    "> 序列最小优化算法(SMO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 优点：泛化错误率低，计算开销不打，结果易于解释\n",
    "- 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改只适用于二分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分隔超平面的形式：$ w^T \\cdot x + b = 0$\n",
    "\n",
    "相应的分类决策函数为$f(x) = sign(w^Tx+b)$\n",
    "\n",
    "### 函数间隔\n",
    "\n",
    "一般来说，一个点距离分隔超平面的远近，可以表示为分类预测的确信程度，在超平面 $  w^T \\cdot x + b = 0$ 确定的情况下，$|w^T \\cdot x + b|$能够相对地表示点`x`距离超平面的远近，而 $ w^T \\cdot x + b$的符号与类标记y的符号是否一致可以表示分类是否正确，则可以用$ y(w^T \\cdot x + b) $来表示分类的正确性和确信程度。\n",
    "\n",
    "**函数间隔**：对于给定训练集`T`和超平面`(w,b)`，定义超平面`(w,b)`关于点$(x_i, y_i)$的函数间隔为\n",
    "\n",
    "$$ \\hat{\\gamma_i} = y_i(w\\cdot x_i+b) $$\n",
    "\n",
    "定义超平面定义超平面`(w,b)`关于训练集`T`的函数间隔为：\n",
    "\n",
    "$$ \\hat{\\gamma} = \\underset{i=1,...,N}{\\mathrm{min}}\\hat{\\gamma_i} $$\n",
    "\n",
    "因为只要成比例的改变`w`和`b`就能够成倍的改变函数间隔，所以我们需要给超平面的法向量`w`加一些约束，比如$||w||=1$,这时函数间隔变成了**几何间隔**：\n",
    "\n",
    "对于给定的训练数据集`T`和超平面`(w,b)`，定义超平面`(w,b)`关于点$(x_i, y_i)$的几何间隔为\n",
    "\n",
    "$$\\gamma_i = y_i(\\frac w {||w||} \\cdot x_i + \\frac b {||w||}) $$\n",
    "\n",
    "定义超平面`(w,b)`关于数据集`T`的几何间隔为：\n",
    "\n",
    "$$ \\gamma = \\underset{1,...,N}{\\mathrm{min}} \\gamma_i $$\n",
    "\n",
    "\n",
    "函数间隔和几何间隔的关系：\n",
    "\n",
    "\n",
    "$$ \\gamma_{i} = \\frac {\\hat{\\gamma_i}} {||w||} $$\n",
    "\n",
    "$$ \\gamma = \\frac {\\hat{\\gamma}} {||w||} $$\n",
    "\n",
    "\n",
    "间隔最大化的直观解释：\n",
    "\n",
    "> 对于训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对数据进行分类，也就是说不仅将正负实例点分开，而且对最难分的点（间隔最小的点）也有足够大的确信度将它们分开，正是这样才让SVM有了很好的泛化能力。\n",
    "\n",
    "最大间隔超平面求解的问题可以表示为：\n",
    "\n",
    "$$ \\underset{w,b}{\\mathrm{max}} \\hspace{3pt} \\gamma$$\n",
    "\n",
    "$$ s.t. y_i ( \\frac {w} {||w||} \\cdot x_i + \\frac {b} {||w||} ) \\ge \\gamma , \\hspace{3pt} 1,2,...,N $$\n",
    "\n",
    "由函数间隔和几何间隔的关系可以得出：\n",
    "\n",
    "$$ \\underset{w,b}{\\mathrm{max}} \\hspace{3pt} \\frac {\\hat{\\gamma}} {||w||} $$\n",
    "\n",
    "$$ s.t. \\hspace{3pt} y_i (w \\cdot x_i + b) \\ge \\hat{\\gamma} , \\hspace{3pt} 1,2,...,N $$\n",
    "\n",
    "\n",
    "对`w`和`b`进行等比例放缩成 $\\lambda w 和 \\lambda b,这时函数间隔变成 {\\lambda} {\\hat{\\gamma}}$\n",
    "\n",
    "因为函数间隔$\\hat{\\gamma}$的取值对最优化问题没有任何影响（因为可以通过同比例的增大`w`和`b`来改变$\\hat{\\gamma}$，所以我们可以取$\\hat{\\gamma}=1$，将其带入上述最优化问题，有因为最大化$\\frac 1 {||w||}$和最小化$\\frac 1 2 ||w||^2$是等价的,所以上述最优化问题等价为：\n",
    "\n",
    "$$ \\underset{w,b}{\\mathrm{min}} \\hspace{3pt} \\frac 1 2 ||w||^2 $$\n",
    "$$ s.t. \\hspace{3pt} y_i (w \\cdot x_i + b) - 1 \\ge 0, \\hspace{3pt} i = 1,2,...,N$$\n",
    "\n",
    "这是一个凸二次规划问题：\n",
    "\n",
    "$$ \\underset{w}{\\mathrm{min}} f(w) $$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray} \n",
    "    s.t. \\hspace{4pt} &g_i(w) \\le 0, &i=1,2,...,N \\\\\n",
    "         &h_i(w) = 0, &i=1,2,...,N\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持向量和间隔边界\n",
    "\n",
    "在线性可分的情况下，训练数据集的样本点与分离超平面距离最近的样本点就叫**支持向量**。\n",
    "\n",
    "由上面的定义可知，支持向量是满足$y_i(w \\cdot x + b) - 1 = 0$ 成立的点。\n",
    "\n",
    "对于$y_i = +1$的正例点，支持向量在超平面$H_1:w \\cdot x + b = 1$。\n",
    "\n",
    "对于$y_i = -1$的负例点，支持向量在超平面$H_2:w \\cdot x + b = -1$\n",
    "\n",
    "$H_1和H_2称为间隔边界。$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习的对偶算法\n",
    "\n",
    "为求解线性可分SVM的最优化问题，将它作为原始最优化问题，\n",
    "应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分SVN的对偶算法。\n",
    "\n",
    "这样的优点是：\n",
    "\n",
    "1. 对偶问题往往更容易求解\n",
    "1. 自然引入核函数，进而推广到非线性问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建拉格朗日函数，并对每个约束引入拉格朗日算子：\n",
    "\n",
    "$$ L(w,b,\\alpha) = {\\frac {1} {2}}||w||^2 - \\sum_{i=1}^{N} \\alpha_iy_i(w \\cdot x + b) + \\sum_{i=1}^{N} \\alpha$$\n",
    "\n",
    "**太多公式了，没时间敲了，看统计学习方法P105页，写的很清楚。。。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platt的SMO算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def loadDataSet(fileName):\n",
    "    dataMat, labelMat = [], []\n",
    "    with open(fileName) as f:\n",
    "        for line in f.readlines():\n",
    "            lineArr = line.strip().split('\\t')\n",
    "            lineArr = list(map(float, lineArr))\n",
    "            dataMat.append(lineArr[:2])\n",
    "            labelMat.append(lineArr[2])\n",
    "    return dataMat, labelMat\n",
    "\n",
    "def selectJrand(i, m):\n",
    "    j = i\n",
    "    while(j == i):\n",
    "        j = random.randint(0, m-1)\n",
    "    return j\n",
    "\n",
    "def clipAlpha(aj, H, L):\n",
    "    if aj > H:\n",
    "        aj = H\n",
    "    if L > aj:\n",
    "        aj = L\n",
    "    return aj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n"
     ]
    }
   ],
   "source": [
    "dataArr, labelArr = loadDataSet('./testSet.txt')\n",
    "print(labelArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smoSimple(dataMatIn, classLabels, C, toler, maxIter):\n",
    "    dataMatrix , labelMat = np.mat(dataMa), np.mat(classLabels).transpose()\n",
    "    b, m ,n = 0, np.shape(dataMatrix)\n",
    "    alphas = mat(zeros(m, 1))\n",
    "    it = 0 \n",
    "    while it < maxIter:\n",
    "        alphaPairsChanged = 0\n",
    "        for i in range(m):\n",
    "            fXi = float(np.multiply(alphas, labelMat).T * \\\n",
    "                        (dataMatrix * dataMatrix[i, :].T) ) + b \n",
    "            Ei = fXi - float(labelMat[i])\n",
    "            if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or \\\n",
    "                ((alphas[i]*Ei > toler) and (alphas[i] > 0)):\n",
    "                    j = selectJrand(i,m)\n",
    "                    fXj = float(np.multiply(alphas, labelMat).T * \\\n",
    "                               (dataMatrix * dataMatrix[j,:].T)) + b\n",
    "                    Ej = fXj - float(labelMat[i])\n",
    "                    alphasIold = alphas[i].copy()\n",
    "                    alphasJold = alphas[j].copy()\n",
    "                    if labelMat[i] != labelMat[j]:\n",
    "                        L = max(0, alphas[j] - alphas[i])\n",
    "                        H = min(C, C + alphas[j] - alphas[i])\n",
    "                    else:\n",
    "                        L = max(0, alphas[j] + alphas[i] - C)\n",
    "                        H = min(C, C + alphas[j] + alphas[i])\n",
    "                    \n",
    "                    if L == H: print('L==H'); continue\n",
    "                    \n",
    "                    eta = 2.0 * dataMatrix[i,:] * dataMatrix[j,:].T - \\\n",
    "                        dataMatrix[i, :] * dataMatrix[i, :] - \\\n",
    "                        dataMatrix[j, :] * dataMatrix[i, :]\n",
    "                    if eat >= 0: print(\"eat >= 0\"); continue\n",
    "                    alhpas[j] -= labelMat[j] * (Ei - Ej) / eta\n",
    "                    alhpas[j] = clipAlpha(alphas[j], H, L)\n",
    "                    if (abs(alphas[j] - alphasJold) < 0.0001):\n",
    "                        print(\"j not moving engoug\")\n",
    "                        continue\n",
    "                    alphas[i] += labelMat[i] * labelMat[j] * (alphasIold - alphas[i])\n",
    "                    b1 = b - Ei - labelMat[i] * (alphas[i] - alphasIold) * \\\n",
    "                        dataMatrix[i, :] * dataMatrix[i, :].T\n",
    "# 看不懂了。。。。\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
